# ğŸ–¼ï¸ Image Caption Generator ğŸ¤–

A **Streamlit** web application that generates intelligent captions and scene descriptions for images using state-of-the-art **Vision-Language AI models** â€” BLIP, CLIP, and ViT â€” powered by HuggingFace Transformers.

![app-demo](https://github.com/yourusername/image-caption-generator/assets/demo.gif)

## ğŸ” Features

âœ… Upload or URL input for any image  
âœ… Generate descriptive captions using **BLIP**  
âœ… Analyze scene and context using **CLIP**  
âœ… Classify objects using **ViT (Vision Transformer)**  
âœ… Combine all outputs into one **enhanced caption**  
âœ… Visualize confidence levels with interactive charts  
âœ… Customizable settings and export results  

---

## ğŸš€ Live Demo

Try it instantly with Streamlit:  
ğŸ‘‰ [Run on Streamlit Cloud](https://share.streamlit.io/yourusername/image-caption-generator)

---

## ğŸ› ï¸ Tech Stack

- **Frontend**: Streamlit (Python UI framework)
- **Backend Models**:
  - ğŸ¤– [BLIP](https://huggingface.co/Salesforce/blip-image-captioning-base) for image captioning
  - ğŸ§  [CLIP](https://huggingface.co/openai/clip-vit-base-patch32) for text-image similarity
  - ğŸ” [ViT](https://huggingface.co/google/vit-base-patch16-224) for object classification
- **Visualization**: Plotly
- **Libraries**: PyTorch, Transformers, PIL, Requests, NumPy, OpenCV

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/yourusername/image-caption-generator.git
cd image-caption-generator
2ï¸âƒ£ Install dependencies
bash
Copy
Edit
pip install -r requirements.txt
If you donâ€™t have a requirements.txt, install manually:

bash
Copy
Edit
pip install streamlit torch torchvision transformers pillow requests numpy opencv-python accelerate plotly
â–¶ï¸ Run the App
bash
Copy
Edit
streamlit run Image_Caption_Generator.py
ğŸ“¸ Screenshot


ğŸ“ Folder Structure
bash
Copy
Edit
image-caption-generator/
â”‚
â”œâ”€â”€ Image_Caption_Generator.py   # Main Streamlit app
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ requirements.txt             # Python dependencies (optional)
â””â”€â”€ assets/                      # (Optional) sample images, demo GIFs
ğŸ’¡ How it Works
Upload or select an image.

The app uses:

BLIP to generate a base caption

CLIP to evaluate scene/text similarity

ViT to classify objects in the image

All outputs are combined into an enhanced caption.

Confidence levels are visualized in interactive charts.

âœ¨ Sample Output
Uploaded Image: Dog on a mountain

BLIP Caption: A dog standing on top of a mountain
Top CLIP Scene: A photo of an animal (98.3%)
ViT Prediction: Eskimo dog (85.6%)
Enhanced Caption:

This appears to be an animal. The image shows a dog standing on top of a mountain with elements suggesting Eskimo dog.

ğŸ“„ Export Feature
ğŸ“ Download a summary of model outputs and enhanced caption as a .txt file directly from the UI.

